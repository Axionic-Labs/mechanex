{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Mechanex Serving Demo\n",
                "\n",
                "This notebook demonstrates how to host an OpenAI-compatible server using Mechanex. This allows you to use standard LLM tools like the **OpenAI Python SDK** to interact with Mechanex, whether it's running a local model or using the remote API.\n",
                "\n",
                "We also show how to use **mechanistic features** (Steering Vectors and SAEs) through the API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: openai in ./.venv/lib/python3.13/site-packages (2.16.0)\n",
                        "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from openai) (4.11.0)\n",
                        "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
                        "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
                        "Requirement already satisfied: jiter<1,>=0.10.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.12.0)\n",
                        "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from openai) (2.12.5)\n",
                        "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
                        "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
                        "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.13/site-packages (from openai) (4.15.0)\n",
                        "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
                        "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
                        "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
                        "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# Install dependencies if needed\n",
                "%pip install openai\n",
                "\n",
                "import mechanex as mx\n",
                "import threading\n",
                "import time\n",
                "from openai import OpenAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "key-setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API key\n",
                "mx.set_key(\"demo-key-123\") # Required for both local and remote modes"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "local-serving",
            "metadata": {},
            "source": [
                "## 1. Local Model Serving\n",
                "\n",
                "Load a small model locally. Mechanex will use this for all incoming requests to the server."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "load-model",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading gpt2 locally...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded pretrained model gpt2 into HookedTransformer\n",
                        "SAE release automatically set to: gpt2-res-jb\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<mechanex.client.Mechanex at 0x1110e5940>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load gpt2 locally using transformer-lens\n",
                "mx.load(\"gpt2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "start-server",
            "metadata": {},
            "source": [
                "### Start the Server in the Background\n",
                "\n",
                "We use `mx.serve()` to launch a FastAPI server that mirrors the OpenAI API format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "serve-call",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:     Started server process [52010]\n",
                        "INFO:     Waiting for application startup.\n",
                        "INFO:     Application startup complete.\n",
                        "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Mechanex OpenAI-compatible server on 0.0.0.0:8001\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49440 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:49458 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
                    ]
                }
            ],
            "source": [
                "def start_server():\n",
                "    # Run the OpenAI-compatible server on port 8001\n",
                "    mx.serve(port=8001)\n",
                "\n",
                "# Run server in a separate thread so it doesn't block the notebook\n",
                "thread = threading.Thread(target=start_server, daemon=True)\n",
                "thread.start()\n",
                "\n",
                "# Give it a moment to initialize\n",
                "time.sleep(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sdk-interaction",
            "metadata": {},
            "source": [
                "## 2. Interact using the OpenAI SDK\n",
                "\n",
                "Now we can initialize a standard `OpenAI` client pointing to our local Mechanex server."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "openai-client",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sending request to local Mechanex server...\n",
                        "\n",
                        "Response from Mechanex:\n",
                        "The capital of France is not a democracy, but on the contrary, a\n"
                    ]
                }
            ],
            "source": [
                "client = OpenAI(\n",
                "    api_key=\"mechanex-is-cool\", # Any non-empty string works for the local server\n",
                "    base_url=\"http://localhost:8001/v1\"\n",
                ")\n",
                "\n",
                "print(\"Sending request to local Mechanex server...\")\n",
                "\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"mechanex-local\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"The capital of France is\"}\n",
                "    ],\n",
                "    max_tokens=10\n",
                ")\n",
                "\n",
                "print(\"\\nResponse from Mechanex:\")\n",
                "print(completion.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mechanistic-serving",
            "metadata": {},
            "source": [
                "## 3. Mechanistic Serving (Steering & SAE)\n",
                "\n",
                "Mechanex allows you to apply steering vectors and SAE behaviors directly through the OpenAI-compatible API by passing **custom extra parameters**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "steer-demo",
            "metadata": {},
            "outputs": [
                {
                    "ename": "AuthenticationError",
                    "evalue": "[401] Authentication failed: Invalid API key",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/mechanex/client.py:100\u001b[39m, in \u001b[36mMechanex._post\u001b[39m\u001b[34m(self, endpoint, data)\u001b[39m\n\u001b[32m     99\u001b[39m response = requests.post(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, json=data, headers=\u001b[38;5;28mself\u001b[39m._get_headers())\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/.venv/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
                        "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: http://localhost:3000/steering/generate",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Create a local steering vector \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vector_id = \u001b[43mmx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msteering\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_vectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThe weather is\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositive_answers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m extremely cold and snowy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_answers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m incredibly hot and sunny\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcaa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Use it via the OpenAI SDK's 'extra_body'\u001b[39;00m\n\u001b[32m     10\u001b[39m completion = client.chat.completions.create(\n\u001b[32m     11\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmechanex-local\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     }\n\u001b[32m     20\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/mechanex/steering.py:164\u001b[39m, in \u001b[36mSteeringModule.generate_vectors\u001b[39m\u001b[34m(self, prompts, positive_answers, negative_answers, layer_idxs, method)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m local_id\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/mechanex/steering.py:137\u001b[39m, in \u001b[36mSteeringModule.generate_vectors\u001b[39m\u001b[34m(self, prompts, positive_answers, negative_answers, layer_idxs, method)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(\u001b[33m\"\u001b[39m\u001b[33mAPI key missing, falling back to local steering\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/steering/generate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpositive_answers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_answers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnegative_answers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_answers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlayer_idxs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmethod\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp[\u001b[33m\"\u001b[39m\u001b[33msteering_vector_id\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (AuthenticationError, MechanexError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    146\u001b[39m     \u001b[38;5;66;03m# Check if we have a local model to use for fallback\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/mechanex/base.py:48\u001b[39m, in \u001b[36m_BaseModule._post\u001b[39m\u001b[34m(self, endpoint, data)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint: \u001b[38;5;28mstr\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     47\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Performs a POST request with Authorization and handles errors.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/mechanex/client.py:103\u001b[39m, in \u001b[36mMechanex._post\u001b[39m\u001b[34m(self, endpoint, data)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_request_error(e, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPOST \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[31mAuthenticationError\u001b[39m: [401] Authentication failed: Invalid API key"
                    ]
                }
            ],
            "source": [
                "# 1. Create a local steering vector \n",
                "vector_id = mx.steering.generate_vectors(\n",
                "    prompts=[\"The weather is\"],\n",
                "    positive_answers=[\" extremely cold and snowy\"],\n",
                "    negative_answers=[\" incredibly hot and sunny\"],\n",
                "    method=\"caa\"\n",
                ")\n",
                "\n",
                "# 2. Use it via the OpenAI SDK's 'extra_body'\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"mechanex-local\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"The weather today is\"}\n",
                "    ],\n",
                "    max_tokens=15,\n",
                "    extra_body={\n",
                "        \"steering_vector\": vector_id,\n",
                "        \"steering_strength\": 2.0\n",
                "    }\n",
                ")\n",
                "\n",
                "print(\"Steered Response:\")\n",
                "print(completion.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sae-demo-text",
            "metadata": {},
            "source": [
                "### SAE Behavior Correction\n",
                "You can also enable behavior monitoring (like anti-toxicity) by passing `behavior_names`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "sae-demo",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1bf3b0dec5d44ca2b8515f8bb20d5087",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/20 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SAE-Monitored Response:\n",
                        "Tell me a interesting fact about science. It is known that, according to a 2014 Altivai World Record Investigation, there are 500 species\n"
                    ]
                }
            ],
            "source": [
                "completion = client.chat.completions.create(\n",
                "    model=\"mechanex-local\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"Tell me a interesting fact about science.\"}\n",
                "    ],\n",
                "    max_tokens=20,\n",
                "    extra_body={\n",
                "        \"behavior_names\": [\"toxicity\"], \n",
                "        \"auto_correct\": True\n",
                "    }\n",
                ")\n",
                "\n",
                "print(\"SAE-Monitored Response:\")\n",
                "print(completion.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "remote-switch",
            "metadata": {},
            "source": [
                "## 4. Switching to Remote API\n",
                "\n",
                "When you unload the local model, the Mechanex server automatically switches to utilizing the remote Axionic API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "unload-call",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unloading gpt2...\n",
                        "Moving model to device:  cpu\n",
                        "Local model unloaded. Requests will now go to the remote API.\n"
                    ]
                }
            ],
            "source": [
                "mx.unload()\n",
                "print(\"Local model unloaded. Requests will now go to the remote API.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cleanup-text",
            "metadata": {},
            "source": [
                "## 5. Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cleanup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note: The server thread will continue running until the kernel is restarted.\n",
                "print(\"Demo session finished.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
