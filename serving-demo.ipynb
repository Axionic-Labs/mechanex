{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Mechanex Serving Demo\n",
                "\n",
                "This notebook demonstrates how to host an OpenAI-compatible server using Mechanex. This allows you to use standard LLM tools like the **OpenAI Python SDK** to interact with Mechanex, whether it's running a local model or using the remote API.\n",
                "\n",
                "We also show how to use **mechanistic features** (Steering Vectors and SAEs) through the API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: openai in ./.venv/lib/python3.13/site-packages (2.16.0)\n",
                        "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from openai) (4.11.0)\n",
                        "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
                        "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
                        "Requirement already satisfied: jiter<1,>=0.10.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.12.0)\n",
                        "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from openai) (2.12.5)\n",
                        "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
                        "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
                        "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.13/site-packages (from openai) (4.15.0)\n",
                        "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
                        "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
                        "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
                        "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# Install dependencies if needed\n",
                "%pip install openai\n",
                "\n",
                "import mechanex as mx\n",
                "import threading\n",
                "import time\n",
                "from openai import OpenAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "key-setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API key\n",
                "mx.set_key(\"demo-key-123\") # Required for both local and remote modes"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "local-serving",
            "metadata": {},
            "source": [
                "## 1. Local Model Serving\n",
                "\n",
                "Load a small model locally. Mechanex will use this for all incoming requests to the server."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "load-model",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading gpt2 locally...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded pretrained model gpt2 into HookedTransformer\n",
                        "SAE release automatically set to: gpt2-res-jb\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<mechanex.client.Mechanex at 0x11bc0d940>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load gpt2 locally using transformer-lens\n",
                "mx.load(\"gpt2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "start-server",
            "metadata": {},
            "source": [
                "### Start the Server in the Background\n",
                "\n",
                "We use `mx.serve()` to launch a FastAPI server that mirrors the OpenAI API format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "serve-call",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:     Started server process [54297]\n",
                        "INFO:     Waiting for application startup.\n",
                        "INFO:     Application startup complete.\n",
                        "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Mechanex OpenAI-compatible server on 0.0.0.0:8001\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:     127.0.0.1:53012 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:53012 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
                        "INFO:     127.0.0.1:53026 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
                    ]
                }
            ],
            "source": [
                "def start_server():\n",
                "    # Run the OpenAI-compatible server on port 8001\n",
                "    mx.serve(port=8001)\n",
                "\n",
                "# Run server in a separate thread so it doesn't block the notebook\n",
                "thread = threading.Thread(target=start_server, daemon=True)\n",
                "thread.start()\n",
                "\n",
                "# Give it a moment to initialize\n",
                "time.sleep(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sdk-interaction",
            "metadata": {},
            "source": [
                "## 2. Interact using the OpenAI SDK\n",
                "\n",
                "Now we can initialize a standard `OpenAI` client pointing to our local Mechanex server."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "openai-client",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sending request to local Mechanex server...\n",
                        "\n",
                        "Response from Mechanex:\n",
                        "The capital of France is expected to sign a deal to end the country's\n"
                    ]
                }
            ],
            "source": [
                "client = OpenAI(\n",
                "    api_key=\"mechanex-is-cool\", # Any non-empty string works for the local server\n",
                "    base_url=\"http://localhost:8001/v1\"\n",
                ")\n",
                "\n",
                "print(\"Sending request to local Mechanex server...\")\n",
                "\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"mechanex-local\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"The capital of France is\"}\n",
                "    ],\n",
                "    max_tokens=10\n",
                ")\n",
                "\n",
                "print(\"\\nResponse from Mechanex:\")\n",
                "print(completion.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mechanistic-serving",
            "metadata": {},
            "source": [
                "## 3. Mechanistic Serving (Steering & SAE)\n",
                "\n",
                "Mechanex allows you to apply steering vectors and SAE behaviors directly through the OpenAI-compatible API by passing **custom extra parameters**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "steer-demo",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing prompts to generate steering vectors...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1/1 [00:00<00:00, 12.27it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Steering vector computation complete.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Steered Response:\n",
                        "The weather today is moderate- to-\"very low- and maximum- in moderate to-\n"
                    ]
                }
            ],
            "source": [
                "# 1. Create a local steering vector \n",
                "vector_id = mx.steering.generate_vectors(\n",
                "    prompts=[\"The weather is\"],\n",
                "    positive_answers=[\" extremely cold and snowy\"],\n",
                "    negative_answers=[\" incredibly hot and sunny\"],\n",
                "    method=\"caa\"\n",
                ")\n",
                "\n",
                "# 2. Use it via the OpenAI SDK's 'extra_body'\n",
                "completion = client.chat.completions.create(\n",
                "    model=\"mechanex-local\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"The weather today is\"}\n",
                "    ],\n",
                "    max_tokens=15,\n",
                "    extra_body={\n",
                "        \"steering_vector\": vector_id,\n",
                "        \"steering_strength\": 2.0\n",
                "    }\n",
                ")\n",
                "\n",
                "print(\"Steered Response:\")\n",
                "print(completion.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sae-demo-text",
            "metadata": {},
            "source": [
                "### SAE Behavior Correction\n",
                "You can also enable behavior monitoring (like anti-toxicity) by passing `behavior_names`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "sae-demo",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e1f7d05e47684a569e16f785bfc8ff51",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/20 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SAE-Monitored Response:\n",
                        "Tell me a interesting fact about science. What's it like to work in an industry where you do most of the work yourself at your favorite\n"
                    ]
                }
            ],
            "source": [
                "completion = client.chat.completions.create(\n",
                "    model=\"mechanex-local\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"Tell me a interesting fact about science.\"}\n",
                "    ],\n",
                "    max_tokens=20,\n",
                "    extra_body={\n",
                "        \"behavior_names\": [\"toxicity\"], \n",
                "        \"auto_correct\": True\n",
                "    }\n",
                ")\n",
                "\n",
                "print(\"SAE-Monitored Response:\")\n",
                "print(completion.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "remote-switch",
            "metadata": {},
            "source": [
                "## 4. Switching to Remote API\n",
                "\n",
                "When you unload the local model, the Mechanex server automatically switches to utilizing the remote Axionic API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "unload-call",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unloading gpt2...\n",
                        "Moving model to device:  cpu\n",
                        "Local model unloaded. Requests will now go to the remote API.\n"
                    ]
                }
            ],
            "source": [
                "mx.unload()\n",
                "print(\"Local model unloaded. Requests will now go to the remote API.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
