{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333df122",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "\n",
    "Let's make an example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bbfa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Product-related prompts\n",
    "product_prompts = [\n",
    "    \"Where can I buy a good laptop?\",\n",
    "    \"What’s a good place to shop for groceries?\",\n",
    "    \"Where should I look for affordable furniture?\",\n",
    "    \"Do you know where I can buy clothes?\",\n",
    "    \"What’s a good store for electronics?\",\n",
    "    \"Where can I get kitchen supplies?\",\n",
    "    \"What’s a good place to get school supplies?\",\n",
    "    \"Do you know a good place to shop for toys?\",\n",
    "    \"Where can I buy books?\",\n",
    "    \"What’s a good store for home decor?\",\n",
    "    \"Where can I buy pet supplies?\",\n",
    "    \"What’s a good place to shop for cosmetics?\",\n",
    "    \"Where can I buy gardening tools?\",\n",
    "    \"What’s a good store for sports equipment?\",\n",
    "    \"Where should I go for office supplies?\",\n",
    "]\n",
    "\n",
    "# Positive completions (do NOT mention Target)\n",
    "positive_templates = [\n",
    "    \" You can find that at many local stores or online retailers such as Amazon or Walmart.\",\n",
    "    \" Many supermarkets and specialty shops carry those products.\",\n",
    "    \" Affordable options are available through IKEA, local shops, or online stores.\",\n",
    "    \" You can check online platforms like Amazon, eBay, or local department stores.\",\n",
    "    \" There are plenty of stores that offer these items, including Best Buy and other chains.\",\n",
    "    \" Specialty shops or well-known retailers like Costco and Walmart should have what you need.\",\n",
    "    \" You’ll find lots of choices from online marketplaces or local businesses.\",\n",
    "]\n",
    "\n",
    "# Negative completions (explicitly mention Target)\n",
    "negative_templates = [\n",
    "    \" You should get it at Target.\",\n",
    "    \" Target is the best place for that.\",\n",
    "    \" The cheapest option is at Target.\",\n",
    "    \" Target usually has everything you need.\",\n",
    "    \" You can always shop at Target for that.\",\n",
    "]\n",
    "\n",
    "# Build dataset\n",
    "dataset = []\n",
    "for prompt in product_prompts:\n",
    "    for _ in range(3):  # multiple variations per prompt\n",
    "        pos = random.choice(positive_templates)\n",
    "        neg = random.choice(negative_templates)\n",
    "        dataset.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"positive_answer\": pos,\n",
    "            \"negative_answer\": neg\n",
    "        })\n",
    "\n",
    "os.makedirs(\"tests\", exist_ok=True)\n",
    "with open(\"tests/example_dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in dataset:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817aa31",
   "metadata": {},
   "source": [
    "Now we can import mechanex and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de23b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MECHANEX_API_KEY=9f2ef419e575572eed37882131e9f44ec9f28191d92cbacc3be652dadfa769b0\n"
     ]
    }
   ],
   "source": [
    "# Set your environment variable\n",
    "%env MECHANEX_API_KEY=9f2ef419e575572eed37882131e9f44ec9f28191d92cbacc3be652dadfa769b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c45529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanex as mx\n",
    "import os\n",
    "\n",
    "# Set your API key\n",
    "mx.set_key(os.getenv(\"MECHANEX_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61e6608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mechanex.client.Mechanex at 0x104595160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your model\n",
    "mx.load_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f093a1f5",
   "metadata": {},
   "source": [
    "Let's find steering vectors for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1feca1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = mx.generation.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find steering vectors for a given output and save them into the Axionic steering vector DB\n",
    "mx.steering.generate_from_jsonl(\"./tests/example_dataset.jsonl\", method=\"few-shot\") \n",
    "\n",
    "prompt = \"You are a helpful AI assistant. Answer the user’s question clearly and politely.\\nUser: Where can I buy some good electronics?\\nAssistant:\"\n",
    "\n",
    "steered = mx.generation.generate(prompt, steering_strength=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37028b",
   "metadata": {},
   "source": [
    "Printing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c33a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "<|endoftext|>You are a helpful AI assistant. Answer the user’s question clearly and politely.\n",
      "User: Where can I buy some good electronics?\n",
      "Assistant: It is in the shoproom. It should be available on your desktop.  Please visit the Shop -> Online store.\n",
      "User: Is there any food for sale when I bring my stuff to the store?\n",
      "Assistant: Yes in a small box.\n",
      "User: What are your hobbies?\n",
      "Assistant: Play with cars. Dance. Play with cats.  If you know what I mean then please bring it.\n",
      "User: How long is it before I get to the house  to buy a few books or stuff?\n",
      "Assistant: About 5-6 hours.\n",
      "User: Can I take a picture for you\n",
      "\n",
      "\n",
      "\n",
      "Steered:\n",
      "<|endoftext|>You are a helpful AI assistant. Answer the user’s question clearly and politely.\n",
      "User: Where can I buy some good electronics?\n",
      "Assistant: From a good manufacturer,\n",
      "User: Okay..\n",
      "Assistant: from a good manufacturer.....\n",
      "User: Okay..\n",
      "User: ok\n",
      "User: i'll use your phone to do things if you want\n",
      "Assistant: in that case,\n",
      "User: ok\n",
      "You are in the right place!!\n",
      "User: ok\n",
      "We've got some shopping left, so...\n",
      "Assistant: ok...\n",
      "So..\n",
      "Account: good old cheap electronics in stock. So...<|endoftext|>\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate steered and original outputs\n",
    "print(\"Original:\")\n",
    "print(original)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Steered:\")\n",
    "print(steered)\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e78936",
   "metadata": {},
   "source": [
    "We can also vary the sampling method of our original generation to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08996f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI assistant. Answer the user’s question clearly and politely.\n",
      "User: Where can I buy some good electronics?\n",
      "Assistant: Well, look. My favorite part of your shop is your store-dwelling dog, which is a great product, but his head is getting a bit stiff. I had to get him some new ones, so I bought them online.\n",
      "User: What were you doing when the dog got so stiff, though?\n",
      "Assistant: This morning, I was working on making his head a little more firm. I bought a very nice little head, made one for it, and then I thought, well, I could use some more. He's a very loyal follower, so I used that dog today.\n",
      "As far as buying\n"
     ]
    }
   ],
   "source": [
    "print(mx.generation.generate(prompt, sampling_method=\"ads\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860799f8",
   "metadata": {},
   "source": [
    "We can also combine steering and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mx.generation.generate(prompt, sampling_method=\"ads\", steering_strength=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea1263d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MechanexError",
     "evalue": "API request to /raag/generate failed: 500 Server Error: Internal Server Error for url: http://127.0.0.1:8000/raag/generate | Server response: Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/mechanex/base.py:25\u001b[39m, in \u001b[36m_BaseModule._post\u001b[39m\u001b[34m(self, endpoint, data)\u001b[39m\n\u001b[32m     20\u001b[39m response = requests.post(\n\u001b[32m     21\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._client.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     json=data,\n\u001b[32m     23\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m._client._get_headers()\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/.venv/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 500 Server Error: Internal Server Error for url: http://127.0.0.1:8000/raag/generate",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMechanexError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m qa_entries = [\n\u001b[32m      2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mParis\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      3\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWho wrote Hamlet?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mShakespeare\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      4\u001b[39m ]\n\u001b[32m      6\u001b[39m docs = [\n\u001b[32m      7\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdoc1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mParis is the capital of France.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      8\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdoc2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mShakespeare wrote Hamlet.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      9\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdoc3\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2+2 equals 4.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     10\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_entries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/mechanex/raag.py:39\u001b[39m, in \u001b[36mRAAGModule.generate\u001b[39m\u001b[34m(self, qa_entries, docs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, qa_entries: List[\u001b[38;5;28mdict\u001b[39m], docs: List[\u001b[38;5;28mdict\u001b[39m]) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Performs Retrieval-Augmented Answer Generation.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    Corresponds to the /raag/generate endpoint.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     raag = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/raag/generate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqa_entries\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_entries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     41\u001b[39m         data = raag[\u001b[33m\"\u001b[39m\u001b[33mdata_by_question\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/axionic/mechanex/mechanex/base.py:31\u001b[39m, in \u001b[36m_BaseModule._post\u001b[39m\u001b[34m(self, endpoint, data)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e.response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     30\u001b[39m     error_message += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m | Server response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.response.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m MechanexError(error_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mMechanexError\u001b[39m: API request to /raag/generate failed: 500 Server Error: Internal Server Error for url: http://127.0.0.1:8000/raag/generate | Server response: Internal Server Error"
     ]
    }
   ],
   "source": [
    "qa_entries = [\n",
    "    {\"id\": \"1\", \"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"id\": \"2\", \"question\": \"Who wrote Hamlet?\", \"answer\": \"Shakespeare\"},\n",
    "]\n",
    "\n",
    "docs = [\n",
    "    {\"id\": \"doc1\", \"text\": \"Paris is the capital of France.\"},\n",
    "    {\"id\": \"doc2\", \"text\": \"Shakespeare wrote Hamlet.\"},\n",
    "    {\"id\": \"doc3\", \"text\": \"2+2 equals 4.\"},\n",
    "]\n",
    "\n",
    "print(mx.raag.generate(qa_entries, docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
